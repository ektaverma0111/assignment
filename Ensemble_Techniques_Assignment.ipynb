{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it. ?\n",
        "\n",
        "Answer =\n",
        "Definition:\n",
        "Ensemble Learning is a technique in machine learning where multiple models (called “weak learners”) are combined to create a stronger, more accurate predictive model. Instead of relying on a single model, the ensemble approach aggregates the predictions of several models to improve accuracy, robustness, and generalization.\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "\n",
        "The core idea is based on the principle that:\n",
        "\n",
        "\"A group of weak models, when combined properly, can outperform a single strong model.\"\n",
        "\n",
        "This works because different models may make different errors, and combining them reduces the overall error through variance reduction, bias reduction, or improved predictions.\n",
        "\n",
        "Why Does It Work?\n",
        "\n",
        "•\tA single model might overfit or underfit the data.\n",
        "\n",
        "•\tMultiple models can capture different aspects of the data.\n",
        "\n",
        "•\tCombining them averages out errors and reduces uncertainty.\n",
        "\n",
        "Types of Ensemble Methods\n",
        "\n",
        "1.\tBagging (Bootstrap Aggregating)\n",
        "    o\tIdea: Train multiple models on different random subsets of the training data and average their predictions.\n",
        "    o\tExample: Random Forest.\n",
        "\n",
        "2.\tBoosting\n",
        "    o\tIdea: Train models sequentially, each new model focusing on the mistakes of the previous one.\n",
        "    o\tExample: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "3.\tStacking\n",
        "    o\tIdea: Combine multiple models using another model (meta-learner) that learns how to best combine their predictions.\n",
        "    o\tExample: Linear model combining outputs of Decision Trees and Neural Networks.\n",
        "\n",
        "Advantages\n",
        "\n",
        "•\tHigher accuracy compared to individual models.\n",
        "\n",
        "•\tReduces overfitting in many cases.\n",
        "\n",
        "•\tHandles complex patterns better.\n"
      ],
      "metadata": {
        "id": "BG12aCPMUk1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer :-  Both Bagging and Boosting are popular ensemble learning techniques used to improve the accuracy and stability of machine learning models, but they differ in their approach.\n",
        "\n",
        "1. Basic Idea\n",
        "\n",
        "•\tBagging (Bootstrap Aggregating):\n",
        "    o\tBuilds multiple independent models on different random subsets of the data (created using bootstrapping).\n",
        "    o\tCombines their predictions by averaging (regression) or majority voting (classification).\n",
        "\n",
        "•\tBoosting:\n",
        "    o\tBuilds models sequentially, where each new model focuses on correcting the errors of the previous models.\n",
        "    o\tFinal prediction is a weighted combination of all models.\n",
        "\n",
        "2. Model Training\n",
        "\n",
        "•\tBagging:\n",
        "    o\tModels are trained in parallel (independent of each other).\n",
        "\n",
        "•\tBoosting:\n",
        "    o\tModels are trained sequentially (each depends on the previous).\n",
        "\n",
        "3. Data Sampling\n",
        "\n",
        "•\tBagging:\n",
        "    o\tUses bootstrap sampling (random samples with replacement).\n",
        "\n",
        "•\tBoosting:\n",
        "    o\tUses the entire dataset, but assigns weights to misclassified points so they get more focus in the next model.\n",
        "\n",
        "4. Error Handling\n",
        "\n",
        "•\tBagging:\n",
        "    o\tReduces variance (helps prevent overfitting).\n",
        "\n",
        "•\tBoosting:\n",
        "    o\tReduces bias (helps improve underfitted models).\n",
        "\n",
        "5. Weights\n",
        "\n",
        "•\tBagging:\n",
        "    o\tAll models have equal weight in final prediction.\n",
        "\n",
        "•\tBoosting:\n",
        "    o\tModels have different weights, based on their accuracy.\n",
        "\n",
        "6. Example Algorithms\n",
        "\n",
        "•\tBagging: Random Forest.\n",
        "\n",
        "•\tBoosting: AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n"
      ],
      "metadata": {
        "id": "frp_ruQiY74s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:  What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:  Bootstrap sampling is a statistical resampling technique where we create multiple new datasets (samples) from the original dataset by sampling with replacement.\n",
        "\n",
        "•\tWith replacement means that after selecting an observation, we put it back before the next draw.\n",
        "\n",
        "•\tAs a result:\n",
        "    o\tEach bootstrap sample has the same size as the original dataset.\n",
        "    o\tSome observations appear multiple times in a sample, while others might not appear at all.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose the original dataset = {1, 2, 3, 4, 5}.\n",
        "\n",
        "A bootstrap sample (with replacement) could be: {2, 3, 2, 5, 1}.\n",
        "\n",
        "Another sample: {4, 4, 1, 3, 5}.\n",
        "\n",
        "Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "Bagging (Bootstrap Aggregating) relies heavily on bootstrap sampling because it:\n",
        "\n",
        "1.\tCreates diversity among models\n",
        "    o\tEach model (e.g., Decision Tree) is trained on a different random bootstrap sample.\n",
        "    o\tThis prevents all models from being identical.\n",
        "\n",
        "2.\tReduces Overfitting\n",
        "    o\tTraining on slightly different datasets means that models capture different patterns.\n",
        "    o\tCombining them by averaging (for regression) or voting (for classification) reduces variance.\n",
        "\n",
        "3.\tForms the basis for Random Forest\n",
        "    o\tIn Random Forest, each tree:\n",
        "\n",
        "Is trained on a bootstrap sample of the dataset.\n",
        "\n",
        "Uses random feature selection at each split for extra diversity.\n",
        "\n",
        "Why is it Important?\n",
        "\n",
        "•\tIf all models see the exact same data, their predictions will be highly correlated.\n",
        "\n",
        "•\tBootstrap sampling ensures independent error patterns across models, making the ensemble more robust.\n",
        "\n",
        "Mathematical Insight\n",
        "\n",
        "If original dataset size = N, each bootstrap sample = N observations.\n",
        "\n",
        "Probability that an observation is not selected in a sample:\n",
        "\n",
        "(1−1N)N≈e−1≈0.368\\left(1 - \\frac{1}{N}\\right)^N \\approx e^{-1} \\approx 0.368\n",
        "(1−N1)N≈e−1≈0.368\n",
        "\n",
        "So about 36.8% of data is not used in each bootstrap sample, called out-of-bag\n",
        "(OOB) samples, which can be used for validation.\n"
      ],
      "metadata": {
        "id": "iPyDM0PeZfk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Anwer:-  Out-of-Bag (OOB) Samples:-\n",
        "\n",
        "•\tIn Bagging methods like Random Forest, we create multiple bootstrap samples (sampling with replacement) from the original dataset for training individual models.\n",
        "\n",
        "•\tEach bootstrap sample is the same size as the original dataset, but since sampling is with replacement:\n",
        "    o\tAbout 63.2% of the original data points appear in a bootstrap sample (on average).\n",
        "    o\tThe remaining 36.8% of data points are not selected in that sample.\n",
        "\n",
        "•\tThese unused data points for a given bootstrap sample are called Out-of-Bag (OOB) samples.\n",
        "\n",
        " OOB Samples are Important:-\n",
        "\n",
        "•  They act as a built-in validation set without needing a separate dataset.\n",
        "\n",
        "•  For every model (e.g., decision tree in Random Forest):\n",
        "\n",
        "•\tThe OOB samples for that model can be used to test its prediction accuracy.\n",
        "\n",
        "•  This is extremely useful because it avoids the need for cross-validation, saving computation.\n",
        "\n",
        "OOB Score:-\n",
        "\n",
        "•\tDefinition:\n",
        "\n",
        "The OOB score is an accuracy estimate for an ensemble model calculated using\n",
        "only the OOB samples.\n",
        "\n",
        "•\tHow it's computed:\n",
        "\n",
        "1.\tFor each observation in the dataset:\n",
        "    Identify the models that did NOT use this observation in training (i.e., where it's OOB).\n",
        "\n",
        "2.\tAggregate predictions from those models.\n",
        "\n",
        "3.\tCompare the aggregated prediction to the actual value.\n",
        "\n",
        "4.\tCompute overall accuracy (classification) or error (regression).\n",
        "\n",
        "•\tFor Random Forest:    OOB Score=Number of correctly predicted OOB samplesTotal number of OOB samples\\text{OOB Score} = \\frac{\\text{Number of correctly predicted OOB samples}}{\\text{Total number of OOB samples}}OOB Score=Total number of OOB samplesNumber of correctly predicted OOB samples\n",
        "Advantages of OOB Score\n",
        "\n",
        "•\tNo need for a separate validation set.\n",
        "\n",
        "•\tProvides an unbiased estimate of model performance because each prediction uses only models that never saw that data point\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose:\n",
        "\n",
        "•\tWe have 1000 training points.\n",
        "\n",
        "•\tFor a specific tree:\n",
        "    o\t632 points used for training (bootstrap sample).\n",
        "    o\t368 points are OOB samples → used for validation.\n",
        "\n",
        "•\tRepeat for all trees and aggregate predictions for each observation.\n"
      ],
      "metadata": {
        "id": "I5m8nhLmZy2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest. ?\n",
        "\n",
        "\n",
        "Answer:-  Feature Importance Analysis: Single Decision Tree vs. Random Forest.\n",
        "\n",
        "1. In a Single Decision Tree\n",
        "\n",
        "•\tHow is Feature Importance Calculated?\n",
        "\n",
        "Feature importance in a Decision Tree is based on how much each feature reduces impurity (e.g., Gini Impurity or Entropy for classification, Variance for regression) across all splits where the feature is used.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1.\tAt each split, calculate the impurity decrease:\n",
        "\n",
        "Decrease in impurity=Impurity (parent)−(Impurity (left child)+Impurity (right\n",
        "child))\\text{Decrease in impurity} = \\text{Impurity (parent)} - \\big( \\text{Impurity (left child)} + \\text{Impurity (right child)} \\big)Decrease in\n",
        "impurity=Impurity (parent)−(Impurity (left child)+Impurity (right child))\n",
        "\n",
        "2.\tAttribute this decrease to the feature used for the split.\n",
        "\n",
        "3.\tSum across all nodes where the feature is used.\n",
        "\n",
        "4.\tNormalize so that all feature importances sum to 1.\n",
        "\n",
        "•\tProperties:\n",
        "    o\tImportance is biased toward features with many unique values (like continuous variables).\n",
        "    o\tResults depend heavily on tree depth and the structure of that single tree.\n",
        "    o\tHigh variance: If the tree changes, importance can shift drastically.\n",
        "\n",
        "2. In a Random Forest\n",
        "\n",
        "•\tHow is Feature Importance Calculated?\n",
        "\n",
        "A Random Forest consists of many Decision Trees. Feature importance is computed\n",
        "by averaging the impurity decreases for each feature across all trees in the forest.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1.\tCompute feature importance in each tree (same way as above).\n",
        "\n",
        "2.\tTake the average (or sum) across all trees.\n",
        "\n",
        "3.\tNormalize so total = 1.\n",
        "\n",
        "•\tWhy is it Better?\n",
        "    o\tReduces variance because it aggregates across many trees.\n",
        "    o\tProvides a more stable and reliable measure of importance.\n",
        "    o\tLess prone to overfitting and random fluctuations.\n"
      ],
      "metadata": {
        "id": "nRGWh2hLaGEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "\n",
        "Answer:-\n",
        "\n",
        "•  Dataset Loading:\n",
        "\n",
        "The load_breast_cancer() function from sklearn.datasets provides a preprocessed dataset for binary classification (Malignant vs Benign tumors).\n",
        "\n",
        "•  Random Forest Classifier:\n",
        "\n",
        "An ensemble algorithm that builds multiple decision trees using bagging and feature randomness to improve accuracy and reduce overfitting.\n",
        "\n",
        "•  Feature Importance in Random Forest:\n",
        "\n",
        "•\tEach feature’s importance is computed as the average impurity decrease across all trees.\n",
        "\n",
        "•\tHigher score = more important in making predictions.\n",
        "Python Code\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X = data.data\n",
        "\n",
        "y = data.target\n",
        "\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Step 2: Train a Random Forest Classifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# Step 3: Get feature importances\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Step 4: Create a DataFrame for better visualization\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances})\n",
        "\n",
        "# Sort by importance in descending order\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Step 5: Print the top 5 most important features\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "\n",
        "print(feature_importance_df.head(5))\n",
        "\n",
        "Expected Output Structure\n",
        "\n",
        "Top 5 Important Features:\n",
        "\n",
        "               Feature  Importance\n",
        "<feature_1>   0.210345\n",
        "\n",
        "<feature_2>   0.180987\n",
        "\n",
        "<feature_3>   0.120543\n",
        "\n",
        "<feature_4>   0.095321\n",
        "\n",
        "<feature_5>   0.070214\n"
      ],
      "metadata": {
        "id": "ZBT8dhD0agu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "\n",
        "Answer:-\n",
        "\n",
        "1.\tIris Dataset\n",
        "    o\tA classic dataset for classification with 150 samples of flowers (3 species).\n",
        "    o\tFeatures: sepal length, sepal width, petal length, petal width.\n",
        "\n",
        "2.\tDecision Tree Classifier\n",
        "    o\tA single tree is prone to high variance and can overfit the training data.\n",
        "\n",
        "3.\tBagging Classifier (Bootstrap Aggregating)\n",
        "    o\tCombines multiple Decision Trees trained on different bootstrap samples.\n",
        "    o\tReduces variance → more stable and accurate than a single tree.\n",
        "\n",
        "4.\tAccuracy Comparison\n",
        "    o\tTrain and test both models using a train-test split.\n",
        "    o\tCompare their accuracy scores.\n",
        "\n",
        "Python Code\n",
        "# Import necessary libraries\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 2: Split the data into train and test sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a single Decision Tree\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Step 4: Train a Bagging Classifier using Decision Trees\n",
        "\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42)\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Step 5: Print accuracies\n",
        "\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_accuracy)\n",
        "\n",
        "print(\"Accuracy of Bagging Classifier:\", bagging_accuracy)\n",
        "\n",
        "Expected Output Example\n",
        "\n",
        "Accuracy of Single Decision Tree: 0.9556\n",
        "\n",
        "Accuracy of Bagging Classifier:   0.9778\n"
      ],
      "metadata": {
        "id": "qcde5BxZa2Gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "\n",
        "\n",
        "Answer:-\n",
        "\n",
        "•  Random Forest Classifier\n",
        "\n",
        "•\tAn ensemble of decision trees using bagging + feature randomness.\n",
        "\n",
        "•\tKey hyperparameters:\n",
        "    o\tn_estimators: Number of trees in the forest.\n",
        "    o\tmax_depth: Maximum depth of each tree (controls overfitting).\n",
        "\n",
        "•  Hyperparameter Tuning with GridSearchCV\n",
        "\n",
        "•\tA method to search over a grid of hyperparameter values.\n",
        "\n",
        "•\tPerforms cross-validation for each combination.\n",
        "\n",
        "•\tReturns the best parameter set based on chosen scoring metric (default:\n",
        "accuracy for classification).\n",
        "\n",
        "•  Steps in the Program\n",
        "\n",
        "•\tLoad dataset (use Iris for simplicity).\n",
        "\n",
        "•\tSplit into train-test sets.\n",
        "\n",
        "•\tDefine parameter grid for n_estimators and max_depth.\n",
        "\n",
        "•\tUse GridSearchCV to train multiple models and find the best.\n",
        "\n",
        "•\tEvaluate accuracy on the test set.\n",
        "\n",
        "Python Code\n",
        "\n",
        "# Import required libraries\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 2: Train-Test Split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize Random Forest Classifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Step 4: Define Hyperparameter Grid\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]}\n",
        "\n",
        "# Step 5: Apply GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5,\n",
        "scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get Best Parameters\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Step 7: Evaluate on Test Data\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Final Accuracy on Test Set:\", final_accuracy)\n",
        "\n",
        " Expected Output Example\n",
        "\n",
        "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
        "\n",
        "Final Accuracy on Test Set: 0.9778"
      ],
      "metadata": {
        "id": "lxhri8PObKxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "\n",
        "Answer:-\n",
        "\n",
        "•  California Housing Dataset\n",
        "\n",
        "•\tA regression dataset predicting house values based on features like income, latitude, longitude, etc.\n",
        "\n",
        "•\tAvailable in sklearn.datasets.fetch_california_housing.\n",
        "\n",
        "•  Bagging Regressor\n",
        "\n",
        "•\tUses bagging with base regressors (e.g., Decision Trees).\n",
        "\n",
        "•\tEach model is trained on a bootstrap sample, and predictions are averaged.\n",
        "\n",
        "•  Random Forest Regressor\n",
        "\n",
        "•\tAn extension of Bagging with extra random feature selection at each split.\n",
        "\n",
        "•\tGenerally performs better than simple Bagging due to decorrelation of trees.\n",
        "\n",
        "•  Mean Squared Error (MSE)\n",
        "\n",
        "•\tUsed to evaluate regression models:\n",
        "\n",
        "MSE=1n∑(yi−y^i)2MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2MSE=n1∑(yi−y^i)2\n",
        "\n",
        "•\tLower MSE = better model.\n",
        "\n",
        "•  Comparison\n",
        "\n",
        "•\tTrain both models on the same train-test split.\n",
        "\n",
        "•\tCompare their MSE values.\n",
        "\n",
        "Python code :-\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load California Housing dataset\n",
        "\n",
        "data = fetch_california_housing()\n",
        "\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Step 2: Train-Test Split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "random_state=42)\n",
        "\n",
        "# Step 3: Train Bagging Regressor\n",
        "\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1)\n",
        "\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "bagging_preds = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Step 4: Train Random Forest Regressor\n",
        "\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1)\n",
        "\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "rf_preds = rf_regressor.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate MSE for both models\n",
        "\n",
        "mse_bagging = mean_squared_error(y_test, bagging_preds)\n",
        "\n",
        "mse_rf = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
        "\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n",
        "\n",
        "\n",
        "Expected Output Example\n",
        "\n",
        "Mean Squared Error (Bagging Regressor): 0.265\n",
        "\n",
        "Mean Squared Error (Random Forest Regressor): 0.22\n"
      ],
      "metadata": {
        "id": "bclsjzdFbaI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan default.  You have access to customer demographic and transaction history data.\n",
        "\n",
        " You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "Answer:-\n",
        "\n",
        "Step-by-Step Approach\n",
        "\n",
        "1. Choose between Bagging or Boosting\n",
        "\n",
        "•\tBagging (e.g., Random Forest)\n",
        "    o\tWorks best when the base model is high variance and low bias, like Decision Trees.\n",
        "    o\tReduces variance by averaging predictions from multiple models trained on different bootstrap samples.\n",
        "\n",
        "•\tBoosting (e.g., XGBoost, LightGBM)\n",
        "    o\tBuilds models sequentially, each correcting errors of the previous one.\n",
        "    o\tReduces bias and can capture complex patterns.\n",
        "    o\tPerforms well on imbalanced datasets (common in loan defaults).\n",
        "\n",
        " Choice:\n",
        "\n",
        "For loan default prediction:\n",
        "\n",
        "•\tIf the dataset is large and complex with class imbalance, Boosting (XGBoost or LightGBM) is preferred because:\n",
        "    o\tHandles non-linear relationships.\n",
        "    o\tAllows weighting of classes (important for rare default cases).\n",
        "\n",
        "•\tBagging (Random Forest) is also good as a baseline.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "•\tBagging:\n",
        "    o\tIncrease n_estimators (more trees) for stability.\n",
        "    o\tLimit tree depth (max_depth) to avoid overly complex trees.\n",
        "    o\tUse max_features to reduce correlation among trees.\n",
        "\n",
        "•\tBoosting:\n",
        "    o\tControl learning rate (smaller = less overfitting).\n",
        "    o\tUse n_estimators carefully (too many = overfitting).\n",
        "    o\tApply early stopping using validation data.\n",
        "    o\tSet max_depth for base learners.\n",
        "\n",
        " General Steps:\n",
        "\n",
        "•\tPerform hyperparameter tuning via GridSearch or RandomSearch.\n",
        "\n",
        "•\tUse cross-validation to detect overfitting early.\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "•\tFor Bagging:\n",
        "    o\tUse Decision Trees as base models (high variance → good for bagging).\n",
        "\n",
        "•\tFor Boosting:\n",
        "    o\tUse shallow trees (stumps) as base learners (to correct bias gradually).\n",
        "\n",
        "4. Evaluate Performance Using Cross-Validation\n",
        "\n",
        "•\tUse Stratified k-Fold Cross-Validation (because the target may be imbalanced).\n",
        "\n",
        "•\tMetrics:\n",
        "    o\tAccuracy (overall performance).\n",
        "    o\tPrecision & Recall (important for defaults, where false negatives are costly).\n",
        "    o\tAUC-ROC (to evaluate model’s ability to separate classes).\n",
        "\n",
        "•\tSteps:\n",
        "\n",
        "1.\tSplit data into k folds.\n",
        "\n",
        "2.\tTrain on k-1 folds, validate on the remaining fold.\n",
        "\n",
        "3.\tAverage performance across folds.\n",
        "\n",
        "5. Justify How Ensemble Learning Improves Decision-Making in This Context\n",
        "\n",
        "•\tWhy Ensemble Helps in Loan Default Prediction:\n",
        "    o\tSingle models (like Decision Trees) may overfit or miss complex patterns.\n",
        "    o\tBagging → reduces variance → more stable predictions.\n",
        "    o\tBoosting → reduces bias → captures subtle patterns in customer behavior.\n",
        "    o\tHandles non-linear relationships in demographic + transaction features.\n",
        "    o\tImproves predictive accuracy → fewer false negatives → better risk management.\n",
        "\n",
        "•\tImpact on Business:\n",
        "    o\tBetter detection of potential defaulters.\n",
        "    o\tReduces financial losses from risky loans.\n",
        "    o\tImproves customer trust by fair and accurate decisions.\n"
      ],
      "metadata": {
        "id": "jNNK3IF-btaP"
      }
    }
  ]
}