{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer:\n",
        "1. Definition of a Decision Tree\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "•\tIt represents decisions in the form of a tree-like structure, where each internal node corresponds to a test on a feature, each branch represents an outcome of that test, and each leaf node represents a final decision or class label.\n",
        "•\tIt mimics human decision-making by breaking down a complex decision-making process into a series of simpler decisions.\n",
        "2. How it works in the context of Classification\n",
        "In classification problems, a Decision Tree is used to assign an input data point to one of the predefined classes. The process involves:\n",
        "1.\tRoot Node Selection\n",
        "o\tThe algorithm starts at the root node with the full dataset.\n",
        "o\tIt chooses the best feature to split the data. The “best” is determined using criteria like:\n",
        "o\tInformation Gain (based on Entropy) – used in ID3 and C4.5 algorithms.\n",
        "o\tGini Index – used in CART (Classification and Regression Tree).\n",
        "2.\tSplitting the Data\n",
        "o\tBased on the chosen feature, the dataset is divided into subsets.\n",
        "o\tFor example, if the feature is “Age,” it may split into groups like “Age < 30” and “Age ≥ 30.”\n",
        "3.\tRecursive Partitioning\n",
        "o\tThe process repeats for each child node using only the subset of data that belongs to that node.\n",
        "o\tThe splitting continues until one of the stopping conditions is met (e.g., maximum depth reached, all samples belong to one class, or no further gain in splitting).\n",
        "4.\tLeaf Nodes\n",
        "o\tWhen no further splitting is possible, a leaf node is created with the final class label.\n",
        "A Decision Tree is a tree-like model used for decision-making. In classification, it splits the dataset step by step based on feature values, until the data points are categorized into a class label. It is one of the most intuitive and widely used algorithms in machine learning."
      ],
      "metadata": {
        "id": "0Z07kmJUs6yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        " Answer:\n",
        "1. What are Impurity Measures in Decision Trees?\n",
        "In Decision Trees, the goal is to split the dataset in such a way that each branch (node) becomes as “pure” as possible.\n",
        "•\tPure Node → All samples belong to one class.\n",
        "•\tImpure Node → Mixed samples (e.g., some “Yes,” some “No”).\n",
        "To decide the best split, impurity measures such as Gini Impurity and Entropy are used.\n",
        "\n",
        "2. Gini Impurity\n",
        "•\tDefinition: Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the node.\n",
        "•\tFormula:\n",
        "Gini=1−∑i=1kpi2Gini = 1 - \\sum_{i=1}^{k} p_i^2Gini=1−i=1∑kpi2\n",
        "Where:\n",
        "•\t_ipi = proportion of samples belonging to class iii\n",
        "•\tkkk = number of classes\n",
        "•\tRange: 0 (pure) to 0.5 (maximum impurity for binary classes).\n",
        "•\tExample:\n",
        "Suppose a node has 10 samples → 7 “Yes” and 3 “No.”\n",
        "p(Yes)=0.7,  p(No)=0.3p(Yes) = 0.7, \\; p(No) = 0.3p(Yes)=0.7,p(No)=0.3 Gini=1−(0.72+0.32)=1−(0.49+0.09)=0.42Gini = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 0.42Gini=1−(0.72+0.32)=1−(0.49+0.09)=0.42\n",
        " Lower Gini means better purity.\n",
        "\n",
        "3. Entropy (Information Gain)\n",
        "•\tDefinition: Entropy measures the disorder or uncertainty in a dataset. A pure node has entropy = 0.\n",
        "•\tFormula:\n",
        "Entropy=−∑i=1kpi⋅log⁡2(pi)Entropy = - \\sum_{i=1}^{k} p_i \\cdot \\log_2(p_i)Entropy=−i=1∑kpi⋅log2(pi)\n",
        "•\tRange: 0 (pure) to 1 (highly impure, in binary classification).\n",
        "•\tExample:\n",
        "For the same node (7 “Yes,” 3 “No”):\n",
        "p(Yes)=0.7,  p(No)=0.3p(Yes) = 0.7, \\; p(No) = 0.3p(Yes)=0.7,p(No)=0.3 Entropy=−(0.7⋅log⁡2(0.7)+0.3⋅log⁡2(0.3))Entropy = -(0.7 \\cdot \\log_2(0.7) + 0.3 `\\cdot \\log_2(0.3))Entropy=−(0.7⋅log2(0.7)+0.3⋅log2(0.3)) =−(0.7⋅−0.515+0.3⋅−1.737)= -(0.7 \\cdot -0.515 + 0.3 \\cdot -1.737)=−(0.7⋅−0.515+0.3⋅−1.737) =0.881= 0.881=0.881\n",
        "Lower entropy means better purity.\n",
        "4. Impact on Splits in a Decision Tree\n",
        "When building a Decision Tree, the algorithm chooses the feature split that produces the greatest reduction in impurity.\n",
        "•\tUsing Gini Impurity (CART algorithm):\n",
        "The algorithm selects the split that minimizes the Gini index.\n",
        "•\tUsing Entropy/Information Gain (ID3, C4.5 algorithms):\n",
        "The algorithm selects the split that maximizes Information Gain, where:\n",
        "Information  Gain=Entropy(parent)−∑nchildnparent⋅Entropy(child)Information \\; Gain = Entropy(parent) - \\sum \\frac{n_{child}}{n_{parent}} \\cdot Entropy(child)InformationGain=Entropy(parent)−∑nparentnchild⋅Entropy(child)"
      ],
      "metadata": {
        "id": "2jtohF3Xtlaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:\n",
        "1. Pruning in Decision Trees\n",
        "•\tPruning is the process of reducing the size of a Decision Tree by removing branches that provide little to no predictive power.\n",
        "•\tIt helps to avoid overfitting and improves the generalization of the model.\n",
        "•\tThere are two main types: Pre-Pruning and Post-Pruning.\n",
        "\n",
        "2. Pre-Pruning (Early Stopping)\n",
        "•\tDefinition: Pre-pruning stops the tree from growing too deep during the construction phase.\n",
        "•\tThe algorithm applies a stopping condition before a node becomes too specific.\n",
        "Common stopping conditions:\n",
        "Maximum depth of the tree.\n",
        "•\tMinimum number of samples required to split a node.\n",
        "•\tMinimum information gain or impurity decrease.\n",
        "Practical Advantage:\n",
        "•\tFaster training time → Since the tree does not grow unnecessarily deep.\n",
        "•\tExample: In a dataset of 10,000 customers, we can stop splitting a node if fewer than 50 customers remain, saving computation and avoiding over-complexity\n",
        "\n",
        "3. Post-Pruning (Prune After Full Growth)\n",
        "•\tDefinition: In post-pruning, the tree is first allowed to grow fully (possibly overfitting), and then the irrelevant branches are removed afterward.\n",
        "•\tThis is done by testing the accuracy of subtrees on a validation set and cutting the branches that do not improve performance.\n",
        "Practical Advantage:\n",
        "•\tBetter accuracy on unseen data → Because pruning removes overfitted branches.\n",
        "•\tExample: A tree predicting whether students pass or fail may overfit by creating deep rules like “Students with 72 marks and 3 absences → Pass.” Post-pruning can cut such overly specific rules and generalize better."
      ],
      "metadata": {
        "id": "_Kv-0ax5ttVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer:\n",
        "1. Definition of Information Gain\n",
        "•\tInformation Gain (IG) is a metric used in Decision Trees to measure how much uncertainty (entropy) is reduced after splitting the dataset on a particular feature.\n",
        "•\tIn simple words: It tells us which feature gives the most useful information to classify the data.\n",
        "2. Formula\n",
        "Information  Gain=Entropy(Parent)−∑i=1knin⋅Entropy(Childi)Information \\; Gain = Entropy(Parent) - \\sum_{i=1}^{k} \\frac{n_i}{n} \\cdot Entropy(Child_i)InformationGain=Entropy(Parent)−i=1∑knni⋅Entropy(Childi)\n",
        "Where:\n",
        "•\tEntropy(Parent)Entropy(Parent)Entropy(Parent) = impurity before the split.\n",
        "•\tEntropy(Childi)Entropy(Child_i)Entropy(Childi) = impurity of each child node.\n",
        "•\tnin\\frac{n_i}{n}nni = proportion of samples in child node iii.\n",
        "The higher the Information Gain, the better the feature is for splitting.\n",
        "\n",
        "3. Practical Example\n",
        "Suppose we want to decide whether students “Pass” or “Fail” based on “Study Hours.”\n",
        "Study Hours\tResult\n",
        ">5\tPass\n",
        ">5\tPass\n",
        "≤5\tFail\n",
        "≤5\tFail\n",
        ">5\tPass\n",
        "•\tStep 1: Parent Node (before split)\n",
        "o\t3 Pass, 2 Fail\n",
        "o\tEntropy = −(35log⁡235+25log⁡225)=0.971-\\left(\\frac{3}{5}\\log_2\\frac{3}{5} + \\frac{2}{5}\\log_2\\frac{2}{5}\\right) = 0.971−(53log253+52log252)=0.971\n",
        "•\tStep 2: Split on “Study Hours”\n",
        "o\tGroup 1 (>5 hrs): 3 Pass, 0 Fail → Entropy = 0\n",
        "o\tGroup 2 (≤5 hrs): 0 Pass, 2 Fail → Entropy = 0\n",
        "•\tStep 3: Weighted Average Entropy\n",
        "35(0)+25(0)=0\\frac{3}{5}(0) + \\frac{2}{5}(0) = 053(0)+52(0)=0\n",
        "•\tStep 4: Information Gain\n",
        "0.971−0=0.9710.971 - 0 = 0.9710.971−0=0.971\n",
        "This is maximum IG, meaning “Study Hours” is the best split feature.\n",
        "\n",
        "4. Why is Information Gain Important?\n",
        "1.\tHelps choose the best feature for splitting the dataset at each node.\n",
        "2.\tReduces impurity → each split makes nodes purer (closer to a single class).\n",
        "3.\tLeads to better accuracy because the tree makes more meaningful splits.\n",
        "4.\tPrevents random or irrelevant splits (e.g., splitting based on “Student’s Shirt Color” would give very low IG)."
      ],
      "metadata": {
        "id": "boee1ylBt5Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "ANSWER\n",
        "1. Real-World Applications of Decision Trees\n",
        "Decision Trees are widely used in both classification and regression tasks across industries:\n",
        "(a) Classification Tasks\n",
        "•\tMedical Diagnosis → Predict whether a patient has a disease based on symptoms (Yes/No).\n",
        "•\tCustomer Churn Prediction → Classify whether a customer will leave a service provider.\n",
        "•\tEmail Spam Detection → Decide whether an email is Spam or Not Spam.\n",
        "•\tCredit Risk Analysis → Approve or reject loan applications based on income, credit score, etc.\n",
        "Example with Iris Dataset (Classification):\n",
        "•\tThe Iris dataset contains features like petal length, petal width, sepal length, sepal width.\n",
        "•\tA Decision Tree can classify a flower into one of the three classes: Iris-setosa, Iris-versicolor, Iris-virginica.\n",
        "•\tThe tree may split on “Petal length” first, because it best separates the classes.\n",
        "\n",
        "(b) Regression Tasks\n",
        "•\tHouse Price Prediction → Predict housing prices based on location, number of rooms, etc.\n",
        "•\tSales Forecasting → Estimate future sales based on historical trends.\n",
        "•\tAgriculture → Predict crop yield based on rainfall, soil quality, and temperature.\n",
        "Example with Boston Housing Dataset (Regression):\n",
        "•\tThe dataset contains features like number of rooms, crime rate, proximity to highway.\n",
        "•\tA Decision Tree Regressor can predict the median value of owner-occupied homes (target variable).\n",
        "•\tExample: A split may happen on “Number of rooms ≥ 6” → Higher predicted price, otherwise → Lower price.\n",
        "\n",
        "2. Main Advantages of Decision Trees\n",
        "1.\tEasy to Understand and Visualize → Works like human decision-making, interpretable even for non-technical users.\n",
        "2.\tHandles Both Numerical and Categorical Data → Flexible for real-world problems.\n",
        "3.\tNo Need for Feature Scaling/Normalization → Unlike SVM or KNN, data preprocessing is minimal.\n",
        "4.\tWorks for Classification & Regression → One algorithm, multiple applications.\n",
        "5.\tFeature Selection Built-in → Automatically selects important features during splitting.\n",
        "\n",
        "3. Main Limitations of Decision Trees\n",
        "1.\tOverfitting → Trees can become too deep and memorize data instead of generalizing.\n",
        "2.\tInstability → Small changes in data may lead to very different trees.\n",
        "3.\tBias Toward Dominant Features → Features with more levels (e.g., many categories) may get chosen unfairly.\n",
        "4.\tLess Accurate Alone → Usually improved with ensemble methods like Random Forest or Gradient Boosted Trees.\n",
        "5.\tContinuous Variables Handling → Can create too many splits if not pruned properly.\n",
        "\n",
        "4. Summary\n",
        "•\tDecision Trees are applied in classification (Iris dataset, medical diagnosis, spam detection) and regression (Boston Housing, price prediction, forecasting).\n",
        "•\tAdvantages: Easy to interpret, versatile, little preprocessing.\n",
        "•\tLimitations: Overfitting, instability, and sometimes lower standalone accuracy."
      ],
      "metadata": {
        "id": "vokxhyUouBMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion ● Print the model’s accuracy and feature importances\n",
        "\n",
        "ANSWER\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target      # Target classes (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# 2. Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Print model’s accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "  \t\tprint(f\"{feature}: {importance:.4f}\")\n",
        "Expected Output (example run)\n",
        "java\n",
        "Copy\n",
        "Edit\n",
        "Model Accuracy: 1.0\n",
        "Feature Importances:\n",
        "sepal length (cm): 0.0000\n",
        "sepal width (cm): 0.0000\n",
        "petal length (cm): 0.4444\n",
        "petal width (cm): 0.5556"
      ],
      "metadata": {
        "id": "vM0HKtIquI2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.?\n",
        "\n",
        "ANSWER\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a fully-grown Decision Tree (no max_depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train a Decision Tree with max_depth = 3\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "\n",
        "# 6. Accuracy Scores\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# 7. Print Results\n",
        "print(\"Accuracy of Fully-Grown Tree:\", accuracy_full)\n",
        "print(\"Accuracy of Tree with max_depth=3:\", accuracy_pruned)\n",
        "\n",
        "Expected Output (example run)\n",
        "Accuracy of Fully-Grown Tree: 1.0\n",
        "Accuracy of Tree with max_depth=3: 0.9667\n"
      ],
      "metadata": {
        "id": "t78Sr6Soujxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Load the California Housing dataset from sklearn\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importance?\n",
        "\n",
        "Answer:\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "#2. Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "#4. Predict on test data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "#5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "#6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "   \t\t print(f\"{feature}: {importance:.4f}\")\n",
        "Expected Output (example run)\n",
        "vbnet\n",
        "Copy\n",
        "Edit\n",
        "Mean Squared Error (MSE): 0.2569\n",
        "\n",
        "Feature Importances:\n",
        "MedInc: 0.6403\n",
        "HouseAge: 0.0427\n",
        "AveRooms: 0.0815\n",
        "AveBedrms: 0.0078\n",
        "Population: 0.0246\n",
        "AveOccup: 0.0142\n",
        "Latitude: 0.0934\n",
        "Longitude: 0.0955\n",
        "(Values may vary slightly due to randomness.)\n",
        "\n",
        "Explanation\n",
        "Dataset: California Housing dataset predicts median house value based on features like median income, house age, average rooms, latitude, longitude, etc.\n",
        "\n",
        "Decision Tree Regressor: Fits the training data and makes continuous predictions.\n",
        "\n",
        "MSE (Mean Squared Error): Measures prediction error → lower is better.\n",
        "\n",
        "Feature Importances: Show which features (e.g., MedInc = Median Income) contribute most to predicting house prices."
      ],
      "metadata": {
        "id": "ukQYEmyWutwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "ANSWER\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]  }\n",
        "\n",
        "# 5. GridSearchCV for tuning hyperparameters\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get best parameters and best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 7. Evaluate on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        " 8. Print Results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n",
        "\n",
        "Expected Output (example run)\n",
        "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
        "Model Accuracy with Best Parameters: 1.0\n",
        "\n",
        "Explanation\n",
        "1.\tGridSearchCV systematically tests combinations of hyperparameters (max_depth, min_samples_split).\n",
        "2.\tIt uses cross-validation (cv=5) to avoid overfitting while tuning.\n",
        "3.\tThe best parameters are selected based on highest accuracy.\n",
        "4.\tFinal model is evaluated on the test set."
      ],
      "metadata": {
        "id": "W-lYoZkxvRqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        " Explain the step-by-step process you would follow to: ● Handle the missing values\n",
        "#● Encode the categorical features\n",
        "#● Train a Decision Tree model\n",
        "#● Tune its hyperparameters\n",
        "#● Evaluate its performance And describe what business value this model could provide in the real-world setting\n",
        "\n",
        "ANSWER\n",
        "Scenario:\n",
        "You are a data scientist in a healthcare company. The goal is to predict whether a patient has a certain disease (Yes/No) using a dataset with mixed data types (numerical + categorical) and some missing values.\n",
        "\n",
        "Step 1: Handle Missing Values\n",
        "    •\tNumerical Features:\n",
        "  \n",
        "  o\tFill missing values with the mean or median (depending on distribution).\n",
        "  \n",
        "  o\tExample: If “Blood Pressure” has missing values, replace them with the median.\n",
        "\n",
        "•\tCategorical Features:\n",
        "\n",
        "o\tFill missing values with the mode (most frequent category).\n",
        "\n",
        "o\tExample: If “Smoking Status” is missing, fill with the most common value (e.g., “Non-Smoker”).\n",
        "\n",
        "•\tAdvanced Option: Use imputation methods (like KNNImputer) if missing values are large.\n",
        "\n",
        "Step 2: Encode Categorical Features\n",
        "\n",
        "•\tSince Decision Trees require numerical input:\n",
        "\n",
        "o\tUse Label Encoding if categories have natural order (e.g., Stage I, Stage II, Stage III).\n",
        "\n",
        "o\tUse One-Hot Encoding if categories are unordered (e.g., Male, Female, Other).\n",
        "\n",
        "Example:   “Smoking Status” → {Non-Smoker, Former Smoker, Current Smoker}\n",
        "\n",
        "→ One-Hot Encode into 3 binary columns.\n",
        "\n",
        "Step 3: Train a Decision Tree Model\n",
        "\n",
        "•\tSplit dataset into training (80%) and testing (20%).\n",
        "\n",
        "•\tTrain a DecisionTreeClassifier with default parameters.\n",
        "\n",
        "•\tCriterion can be \"gini\" or \"entropy\".\n",
        "\n",
        "Python Example:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "Step 4: Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV to optimize:\n",
        "\n",
        "•\tmax_depth → prevent overfitting.\n",
        "\n",
        "•\tmin_samples_split → minimum samples needed to split a node.\n",
        "\n",
        "•\tmin_samples_leaf → minimum samples in leaf node.\n",
        "\n",
        "•\tcriterion → Gini or Entropy.\n",
        "\n",
        "Python Example:\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [3, 5, 10, None],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"criterion\": [\"gini\", \"entropy\"]  }\n",
        "\n",
        "GridSearch selects the best combination for maximum accuracy.\n",
        "\n",
        "Step 5: Evaluate Model Performance\n",
        "\n",
        "•\tAccuracy Score → % of correct predictions.\n",
        "\n",
        "•\tPrecision & Recall → Important in healthcare (recall ensures sick patients\n",
        "are not missed).\n",
        "\n",
        "•\tF1-Score → Balances precision & recall.\n",
        "\n",
        "•\tConfusion Matrix → Shows true positives, false negatives (very important in disease prediction).\n",
        "\n",
        "Example:\n",
        "\n",
        "•\tIf accuracy = 92%, recall = 95% → model successfully detects most diseased patients.\n",
        "\n",
        "Step 6: Business Value in Real-World Setting\n",
        "\n",
        "•\tEarly Disease Detection: Helps doctors identify at-risk patients quickly.\n",
        "\n",
        "•\tCost Reduction: Reduces unnecessary tests by focusing on high-risk patients.\n",
        "\n",
        "•\tDecision Support System: Assists medical staff in diagnosis.\n",
        "\n",
        "•\tImproved Patient Outcomes: Faster treatment for correctly predicted positive cases.\n",
        "\n",
        "•\tScalability: Can be applied across hospitals with large patient databases.\n",
        "\n",
        "Example:\n",
        "\n",
        "If the model predicts with 95% recall, it ensures that almost all patients with the disease are identified → potentially saves lives by enabling early treatment."
      ],
      "metadata": {
        "id": "aCTlfORBvr0h"
      }
    }
  ]
}