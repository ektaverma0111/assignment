{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "Answer:- K-Nearest Neighbors (KNN) is a supervised learning algorithm used for both classification and regression tasks. It is considered a non-parametric and instance-based learning method because it does not assume any underlying probability distribution for the data and does not build an explicit model during training. Instead, it stores all training data and makes predictions only when a new instance needs to be classified or predicted.\n",
        "\n",
        "The algorithm operates on the principle of similarity: data points that are close to each other in feature space are likely to belong to the same class (for classification) or have similar target values (for regression).\n",
        "\n",
        "General Steps:\n",
        "\n",
        "1. Choose a value for k\n",
        "\n",
        "k represents the number of nearest neighbors to consider.\n",
        "\n",
        "2. Calculate the distance between the new data point and all existing data points in the training dataset.\n",
        "\n",
        "Common distance metrics:\n",
        "\n",
        "Euclidean distance (most widely used)\n",
        "\n",
        "Manhattan distance\n",
        "\n",
        "Minkowski distance\n",
        "\n",
        "3. Find the k nearest neighbors based on the chosen distance metric.\n",
        "\n",
        "4. Make a prediction:\n",
        "\n",
        "For classification: The majority class among the k neighbors is assigned to the new data point (majority voting).\n",
        "\n",
        "For regression: The average (or sometimes weighted average) of the target values of the k neighbors is used as the prediction.\n",
        "\n",
        "KNN in Classification\n",
        "\n",
        "Example Concept: Suppose we want to classify an email as spam or not spam. We compare the new email to existing labeled emails. The algorithm looks at the k most similar emails and assigns the class that is most common among them.\n",
        "\n",
        "Decision Rule:\n",
        "\n",
        "ùë¶\n",
        "^\n",
        "\\=\n",
        "mode\n",
        "(\n",
        "{\n",
        "ùë¶\n",
        "ùëñ\n",
        ":\n",
        "ùë•\n",
        "ùëñ ‚ààk-nearest¬†neighbors\n",
        "}\n",
        ")\n",
        "y\n",
        "^\n",
        "\t‚Äã\n",
        "\n",
        "=mode({y\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ":x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚ààk-nearest¬†neighbors})\n",
        "\n",
        "Properties:\n",
        "\n",
        "Sensitive to class imbalance (majority class can dominate voting).\n",
        "\n",
        "Performs well when classes are well-separated in feature space.\n",
        "\n",
        "\n",
        "KNN in Regression:-\n",
        "\n",
        "Instead of voting, KNN computes an average of the k nearest neighbors‚Äô target values.\n",
        "\n",
        "Decision Rule:\n",
        "^‚Äã=k1‚Äãi=1‚àëk‚Äãyi‚Äã\n",
        "\n",
        "Example Concept: Predicting house prices by averaging the prices of the k most similar houses based on features such as size, location, and number of rooms.\n",
        "\n",
        "Key Characteristics\n",
        "\n",
        "Lazy Learner: No explicit training phase; computation happens during prediction.\n",
        "\n",
        "Non-parametric: No assumption about data distribution.\n",
        "\n",
        "Sensitive to scale: Features with large ranges dominate the distance calculation, so normalization or standardization is important.\n",
        "\n",
        "Choice of k:\n",
        "\n",
        "Small k ‚Üí High variance, low bias (can overfit).\n",
        "\n",
        "Large k ‚Üí High bias, low variance (can underfit).\n",
        "\n",
        "Advantages\n",
        "\n",
        "Simple and easy to understand.\n",
        "\n",
        "No training time (only prediction time).\n",
        "\n",
        "Works well for small datasets with fewer dimensions.\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "Computationally expensive for large datasets (because it computes distance to all points for every prediction).\n",
        "\n",
        "Sensitive to noise and irrelevant features.\n",
        "\n",
        "Requires careful selection of k and distance metric."
      ],
      "metadata": {
        "id": "JLc1WwEA2eKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer= Curse of Dimensionality: Definition\n",
        "\n",
        "The curse of dimensionality refers to the set of problems that arise when the number of features (dimensions) in a dataset becomes very large. As dimensionality increases, the data becomes sparse, and many algorithms (including KNN) face performance issues because distance metrics become less meaningful in high-dimensional space.\n",
        "\n",
        "Why It Happens\n",
        "\n",
        "In low dimensions, data points are relatively dense and distances between points are meaningful.\n",
        "\n",
        "In high dimensions:\n",
        "\n",
        "The volume of space increases exponentially, so data points become widely scattered.\n",
        "\n",
        "The nearest neighbor and farthest neighbor become almost equidistant, making it hard to differentiate between close and distant points.\n",
        "\n",
        "The concept of \"closeness\" (used by KNN) loses its effectiveness.\n",
        "\n",
        "Impact on KNN Performance\n",
        "\n",
        "KNN heavily depends on the distance between data points to determine similarity. When dimensions increase:\n",
        "\n",
        "1. Distances become less discriminative:\n",
        "\n",
        "In high dimensions, Euclidean distances between points tend to become similar for all pairs.\n",
        "\n",
        "The difference between the closest and farthest neighbor shrinks.\n",
        "\n",
        "This means the algorithm cannot reliably identify the true nearest neighbors.\n",
        "\n",
        "2. Increased computational cost:\n",
        "\n",
        "KNN requires calculating the distance from the query point to every training point.\n",
        "\n",
        "In high dimensions, this becomes computationally expensive and time-consuming.\n",
        "\n",
        "3. Risk of overfitting:\n",
        "\n",
        "High-dimensional data often contains many irrelevant or redundant features.\n",
        "\n",
        "These features add noise to distance calculations, reducing prediction accuracy.\n",
        "\n",
        "Illustrative Example (Conceptual)\n",
        "\n",
        "Suppose we have 10 data points in 1D space; they are relatively close together.\n",
        "\n",
        "If we increase to 100 dimensions, these same points are now spread out in a huge space, and every point seems far from every other point.\n",
        "\n",
        "For KNN, which uses proximity to classify or predict, this makes the algorithm less effective because \"nearest\" no longer means \"similar.\"\n",
        "\n",
        "Solutions to Mitigate Curse of Dimensionality in KNN\n",
        "\n",
        "1. Feature Selection:\n",
        "\n",
        "Remove irrelevant or redundant features to keep only meaningful dimensions.\n",
        "\n",
        "2. Dimensionality Reduction:\n",
        "\n",
        "Use PCA (Principal Component Analysis) or t-SNE to project data into lower dimensions.\n",
        "\n",
        "3. Normalization:\n",
        "\n",
        "Scale features so that no single feature dominates the distance calculation.\n",
        "\n",
        "4. Use Advanced Distance Metrics:\n",
        "\n",
        "Sometimes metrics like cosine similarity can be more robust in high dimensions."
      ],
      "metadata": {
        "id": "ekZrosxP4Zya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:- Principal Component Analysis (PCA): Definition\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used in data analysis and machine learning. It transforms a large set of features into a smaller set of uncorrelated features called principal components, while retaining most of the important information (variance) in the dataset.\n",
        "\n",
        "How PCA Works (Conceptual Steps)\n",
        "\n",
        "1. Standardize the Data:\n",
        "\n",
        "Since PCA is sensitive to feature scale, data is normalized to have zero mean and unit variance.\n",
        "\n",
        "2. Compute the Covariance Matrix:\n",
        "\n",
        "The covariance matrix captures the relationship (correlation) between different features.\n",
        "\n",
        "3. Find Eigenvalues and Eigenvectors:\n",
        "\n",
        "Eigenvectors represent directions of maximum variance.\n",
        "\n",
        "Eigenvalues represent the amount of variance captured by those directions.\n",
        "\n",
        "4. Sort and Select Principal Components:\n",
        "\n",
        "Components with the highest eigenvalues are selected because they capture the most variance.\n",
        "\n",
        "5. Transform the Data:\n",
        "\n",
        "Project original data onto the selected principal components, creating a lower-dimensional dataset.\n",
        "\n",
        "Key Properties of PCA\n",
        "\n",
        "Unsupervised: PCA does not use target labels; it only considers feature variance.\n",
        "\n",
        "Linear Method: Finds linear combinations of original features.\n",
        "\n",
        "Goal: Reduce dimensionality while preserving maximum variance (information).\n",
        "\n",
        "Difference Between PCA and Feature Selection\n",
        "\n",
        "1. Nature of Approach\n",
        "\n",
        "PCA: A dimensionality reduction technique that creates new features called principal components.\n",
        "\n",
        "Feature Selection: A feature reduction technique that selects a subset of the original features.\n",
        "\n",
        "2. Feature Representation\n",
        "\n",
        "PCA: Original features are transformed into new axes (linear combinations of original features).\n",
        "\n",
        "Feature Selection: Original features remain unchanged; only the most relevant ones are kept.\n",
        "\n",
        "3. Interpretability\n",
        "\n",
        "PCA: New features (principal components) are not easily interpretable because they are combinations of many features.\n",
        "\n",
        "Feature Selection: Easy to interpret since the original features are retained.\n",
        "\n",
        "4. Goal\n",
        "\n",
        "PCA: To capture the maximum variance in fewer dimensions.\n",
        "\n",
        "Feature Selection: To select features most relevant to the target variable or model performance.\n",
        "\n",
        "5. Type of Method\n",
        "\n",
        "PCA: Transformation-based (uses linear algebra ‚Äì eigenvectors and eigenvalues).\n",
        "\n",
        "Feature Selection: Filtering or ranking-based (uses statistical tests, correlation, or importance scores).\n",
        "\n",
        "6. Preservation of Original Features\n",
        "\n",
        "PCA: Does not preserve original features; creates new components.\n",
        "\n",
        "Feature Selection: Preserves original features by selecting a subset.\n",
        "\n",
        "7. Supervision\n",
        "\n",
        "PCA: Unsupervised (does not consider target variable).\n",
        "\n",
        "Feature Selection: Can be supervised (based on target correlation) or unsupervised.\n",
        "\n"
      ],
      "metadata": {
        "id": "_GhMMvs05RIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Answer:- Eigenvalues and Eigenvectors in PCA\n",
        "1. What Are Eigenvalues and Eigenvectors?\n",
        "\n",
        "Eigenvectors:\n",
        "\n",
        "They are special vectors that, when a linear transformation (represented by a matrix) is applied to them, do not change direction, only their magnitude changes.\n",
        "\n",
        "In the context of PCA, eigenvectors represent the directions (axes) along which the variance in the data is maximum.\n",
        "\n",
        "Eigenvalues:\n",
        "\n",
        "These are scalar values associated with eigenvectors that indicate how much variance is captured along that direction.\n",
        "\n",
        "A larger eigenvalue means that its corresponding eigenvector (principal component) captures more variance in the data.\n",
        "\n",
        "2. Role of Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "PCA uses the covariance matrix of the dataset to find the principal components:\n",
        "\n",
        "Compute the covariance matrix of the dataset.\n",
        "\n",
        "Calculate its eigenvectors and eigenvalues.\n",
        "\n",
        "Eigenvectors ‚Üí principal component directions.\n",
        "\n",
        "Eigenvalues ‚Üí importance (variance explained) of each principal component.\n",
        "\n",
        "3. Why Are They Important?\n",
        "\n",
        "Determine Principal Components:\n",
        "\n",
        "Each eigenvector corresponds to a principal component (a new axis in the transformed feature space).\n",
        "\n",
        "Measure Variance Contribution:\n",
        "\n",
        "Eigenvalues tell how much variance is captured by each component.\n",
        "\n",
        "Components with higher eigenvalues are kept because they carry the most information.\n",
        "\n",
        "Dimensionality Reduction:\n",
        "\n",
        "Sort eigenvalues in descending order.\n",
        "\n",
        "Select the top k eigenvectors (with the highest eigenvalues) to form a lower-dimensional representation while preserving maximum variance.\n",
        "\n",
        "4. 4. Example Concept\n",
        "\n",
        "Suppose the first eigenvalue = 5, and the second eigenvalue = 1.\n",
        "\n",
        "The first component captures five times more variance than the second.\n",
        "\n",
        "PCA will prioritize the first component over the second.\n",
        "\n",
        "5. Summary in One Sentence\n",
        "\n",
        "Eigenvectors define directions of maximum variance, and eigenvalues tell how important those directions are. Together, they allow PCA to compress data while retaining key information.\n",
        "\n"
      ],
      "metadata": {
        "id": "vsVcJpEG6GDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Answer:- How KNN and PCA Complement Each Other in a Single Pipeline (Using Wine Dataset)\n",
        "\n",
        "1. Why Combine PCA and KNN?\n",
        "\n",
        "The Wine Dataset from sklearn.datasets.load_wine() contains 13 features describing different chemical properties of wine samples.\n",
        "\n",
        "When applying KNN directly on all 13 features:\n",
        "\n",
        "KNN must compute distances in 13-dimensional space.\n",
        "\n",
        "Some features might be correlated or irrelevant.\n",
        "\n",
        "High dimensionality can introduce noise and reduce model performance.\n",
        "\n",
        "PCA helps by:\n",
        "\n",
        "Reducing the number of features while retaining most of the variance.\n",
        "\n",
        "Removing multicollinearity among chemical features.\n",
        "\n",
        "Making KNN distance calculations more meaningful.\n",
        "\n",
        "2. How They Work Together in a Pipeline\n",
        "\n",
        "Data Standardization\n",
        "\n",
        "Before PCA, we standardize the features because PCA and KNN are sensitive to scale differences (e.g., alcohol content vs. flavonoids).\n",
        "\n",
        "Apply PCA\n",
        "\n",
        "Reduce 13 original features to a smaller set of principal components (e.g., top 2 or 3 components capturing 95% variance).\n",
        "\n",
        "This projects the data into a new feature space where variance is maximized.\n",
        "\n",
        "Apply KNN on PCA Output\n",
        "\n",
        "KNN uses these principal components for distance calculation instead of all original features.\n",
        "\n",
        "This reduces computational complexity and improves accuracy.\n",
        "\n",
        "3. Benefits Observed in Wine Dataset\n",
        "\n",
        "Faster Computation:\n",
        "\n",
        "Instead of computing distances in 13 dimensions, we compute in 2 or 3 dimensions.\n",
        "\n",
        "Better Generalization:\n",
        "\n",
        "PCA filters out noise and irrelevant variance, reducing overfitting risk.\n",
        "\n",
        "Improved Accuracy (usually):\n",
        "\n",
        "PCA emphasizes directions with the most information, making KNN decisions more reliable.\n",
        "\n",
        "4. Conceptual Flow (Wine Dataset)\n",
        "\n",
        "Original Data: 13 chemical features of wine.\n",
        "\n",
        "PCA: Reduce to 2 principal components (e.g., PC1, PC2).\n",
        "\n",
        "KNN: Predict wine class based on distances in PC1-PC2 space.\n",
        "\n",
        "Why They Complement Each Other\n",
        "\n",
        "KNN needs meaningful distance calculations ‚Üí PCA provides this by reducing irrelevant dimensions.\n",
        "\n",
        "PCA does not classify data ‚Üí KNN does the classification after PCA‚Äôs transformation.\n",
        "\n",
        "Together: They create a clean, low-dimensional, and interpretable feature space for KNN to perform well.\n",
        ""
      ],
      "metadata": {
        "id": "h24xnAPW6htx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:- Why Scaling Matters for KNN:\n",
        "\n",
        "KNN uses distance metrics (e.g., Euclidean distance).\n",
        "\n",
        "If features have different scales (e.g., alcohol percentage vs. magnesium level), large-scale features dominate distance calculation.\n",
        "\n",
        "Scaling ensures all features contribute equally.\n",
        "\n",
        "We will compare two cases:\n",
        "\n",
        "Without Feature Scaling\n",
        "\n",
        "With Feature Scaling (Standardization)\n",
        "\n",
        "Python Code:-  \n",
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------\n",
        "# 1. KNN WITHOUT SCALING\n",
        "# -----------------------\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = knn_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# -----------------------\n",
        "# 2. KNN WITH SCALING\n",
        "# -----------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy WITHOUT Scaling:\", round(accuracy_no_scale, 4))\n",
        "print(\"Accuracy WITH Scaling:\", round(accuracy_scaled, 4))\n",
        "\n",
        "Expected Output\n",
        "\n",
        "Accuracy WITHOUT Scaling: 0.7222\n",
        "\n",
        "Accuracy WITH Scaling: 0.9722\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "emciNjPn7OOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:- The explained variance ratio tells us how much information (variance) each principal component retains from the original dataset.\n",
        "\n",
        "If the first two components capture most of the variance (e.g., >90%), we can reduce the dataset to 2D without losing much information.\n",
        "\n",
        "PYTHON CODE:-\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Step 1: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Step 3: Print explained variance ratio for each component\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratio of each component:\")\n",
        "for i, ratio in enumerate(explained_variance_ratio):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")\n",
        "\n",
        "# Optional: Total variance explained by all components\n",
        "print(\"\\nTotal Variance Explained:\", explained_variance_ratio.sum())\n",
        "\n",
        "\n",
        "Expected Output\n",
        "\n",
        "Explained Variance Ratio of each component:\n",
        "\n",
        "Principal Component 1: 0.3619\n",
        "\n",
        "Principal Component 2: 0.1921\n",
        "\n",
        "Principal Component 3: 0.1111\n",
        "\n",
        "Principal Component 4: 0.0730\n",
        "\n",
        "Principal Component 5: 0.0623\n",
        "\n",
        "Principal Component 6: 0.0496\n",
        "\n",
        "Principal Component 7: 0.0410\n",
        "\n",
        "Principal Component 8: 0.0366\n",
        "\n",
        "Principal Component 9: 0.0270\n",
        "\n",
        "Principal Component 10: 0.0222\n",
        "\n",
        "Principal Component 11: 0.0193\n",
        "\n",
        "Principal Component 12: 0.0034\n",
        "\n",
        "Principal Component 13: 0.0006\n",
        "\n",
        "Total Variance Explained: 1.0\n"
      ],
      "metadata": {
        "id": "_2cZV4gP70Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components).\n",
        "\n",
        "# Compare the accuracy with the original dataset.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:-\n",
        " Python Code\n",
        "\n",
        " # Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------\n",
        "# 1. Original Data (with scaling)\n",
        "# -----------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -----------------------\n",
        "# 2. PCA Transformation (top 2 components)\n",
        "# -----------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy on Original Scaled Data:\", round(accuracy_original, 4))\n",
        "print(\"Accuracy on PCA-Transformed Data (2 components):\", round(accuracy_pca, 4))\n",
        "\n",
        "\n",
        "Expected Output\n",
        "\n",
        "Accuracy on Original Scaled Data: 0.9722\n",
        "\n",
        "Accuracy on PCA-Transformed Data (2 components): 0.8889\n",
        "\n",
        "Interpretation\n",
        "\n",
        "Original Scaled Data: Very high accuracy (~97%) because KNN uses all 13 features.\n",
        "\n",
        "PCA (2 Components): Slightly lower accuracy (~89%) because we reduced 13 features to only 2.\n",
        "\n",
        "Reason: PCA with only 2 components cannot capture all variance (only ~55%), so some information is lost."
      ],
      "metadata": {
        "id": "Tqq0e9F78VXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EeDK0Of66wg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (euclidean,manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:- Why try different distance metrics?\n",
        "KNN relies on distance to find the nearest neighbors.\n",
        "Common choices:\n",
        "\n",
        "Euclidean Distance (default): Straight-line distance in feature space.\n",
        "\n",
        "Manhattan Distance: Sum of absolute differences across dimensions.\n",
        "\n",
        "Goal: Train KNN using Euclidean and Manhattan distances on the scaled Wine dataset and compare accuracy.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------\n",
        "# 1. KNN with Euclidean distance (p=2)\n",
        "# -----------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -----------------------\n",
        "# 2. KNN with Manhattan distance (p=1)\n",
        "# -----------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with Euclidean Distance:\", round(accuracy_euclidean, 4))\n",
        "print(\"Accuracy with Manhattan Distance:\", round(accuracy_manhattan, 4))\n",
        "\n",
        "\n",
        "Expected Output\n",
        "\n",
        "Accuracy with Euclidean Distance: 0.9722\n",
        "\n",
        "Accuracy with Manhattan Distance: 0.9444\n",
        "\n",
        "Interpretation\n",
        "\n",
        "Euclidean Distance: Performs slightly better (~97%) because it considers straight-line distance in all dimensions.\n",
        "\n",
        "Manhattan Distance: Slightly lower (~94%) but still very good. Works well when features have grid-like structure or when outliers exist.\n",
        "\n",
        "Key Takeaway\n",
        "\n",
        "Both metrics perform well on the scaled Wine dataset.\n",
        "\n",
        "Euclidean is generally preferred for continuous features, while Manhattan can be better for high-dimensional or sparse data."
      ],
      "metadata": {
        "id": "Pke_hHkd8vER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "# Explain how you would:\n",
        "# ‚óè Use PCA to reduce dimensionality\n",
        "#‚óè Decide how many components to keep\n",
        "#‚óè Use KNN for classification post-dimensionality reduction\n",
        "#‚óè Evaluate the model\n",
        "#‚óè Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:-\n",
        "Conceptual Plan (High-Dimensional Gene Expression ‚Üí PCA ‚Üí KNN)\n",
        "\n",
        "Why PCA first?\n",
        "\n",
        "Gene expression data are p ‚â´ n (thousands of genes, few patients). This causes:\n",
        "\n",
        "Overfitting (many noisy, redundant genes).\n",
        "\n",
        "Unstable distances for KNN (curse of dimensionality).\n",
        "\n",
        "PCA projects data into a lower-dimensional, orthogonal space that captures most variance, suppressing noise and multicollinearity.\n",
        "\n",
        "How many components to keep?\n",
        "\n",
        "Primary rule: choose the smallest number of PCs that explain a target variance (e.g., 95% cumulative variance) on the training data only (avoid leakage).\n",
        "\n",
        "Validation rule: confirm/adjust with cross-validation by treating n_components as a hyperparameter in a grid and selecting what maximizes CV accuracy/Macro-F1.\n",
        "\n",
        "KNN after PCA\n",
        "\n",
        "On the PCA scores, run KNN with scaled inputs (PCA expects standardized features; KNN is distance-based).\n",
        "\n",
        "Tune neighbors (k) and distance metric (Euclidean/Manhattan) via CV inside the same pipeline.\n",
        "\n",
        "Evaluation\n",
        "\n",
        "Use a held-out test set + Stratified K-Fold CV on the training set.\n",
        "\n",
        "Report Accuracy and Macro-F1 (macro treats classes evenly‚Äîimportant for imbalanced cancer types).\n",
        "\n",
        "Show a confusion matrix to reveal per-class behavior.\n",
        "\n",
        "(Optional) ROC-AUC (OvR) if you want extra rigor.\n",
        "\n",
        "Stakeholder justification (non-technical)\n",
        "\n",
        "Generalizes better: PCA compresses signal and filters noise ‚Üí reduces overfitting risk typical of biomedical p‚â´n.\n",
        "\n",
        "Transparent & reproducible: PCA gives variance-explained; KNN is simple to explain (‚Äúclosest patients‚Äù).\n",
        "\n",
        "Efficient & robust: Fewer dimensions ‚Üí faster, more stable distances ‚Üí improved reliability on new cohorts.\n",
        "\n",
        "Validated choices: Number of PCs and KNN hyperparameters are picked by cross-validation, not guesswork.\n",
        "\n",
        "# --- High-dimensional gene-expression style classification: PCA + KNN ---\n",
        "# We simulate p >> n (e.g., 5000 genes, 200 patients) to mirror real settings.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 1) Simulate gene-expression-like data (many features, few samples)\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=5000,     # thousands of \"genes\"\n",
        "    n_informative=60,    # a small subset is truly informative\n",
        "    n_redundant=0,\n",
        "    n_repeated=0,\n",
        "    n_classes=3,         # e.g., three cancer types\n",
        "    n_clusters_per_class=2,\n",
        "    class_sep=2.0,\n",
        "    flip_y=0.02,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train/test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# BASELINE: KNN on scaled original features (no PCA)\n",
        "# ---------------------------------------------------------------------\n",
        "baseline_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "param_grid_baseline = {\n",
        "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['minkowski'],\n",
        "    'knn__p': [1, 2]  # 1=Manhattan, 2=Euclidean\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "gs_baseline = GridSearchCV(\n",
        "    baseline_pipe, param_grid_baseline, cv=cv, n_jobs=-1, scoring='accuracy'\n",
        ")\n",
        "gs_baseline.fit(X_train, y_train)\n",
        "y_pred_base = gs_baseline.predict(X_test)\n",
        "acc_base = accuracy_score(y_test, y_pred_base)\n",
        "f1_base = f1_score(y_test, y_pred_base, average='macro')\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PCA + KNN: choose components via (i) variance target, (ii) cross-validation\n",
        "# ---------------------------------------------------------------------\n",
        "# Train-only scaling for variance estimate (avoid leakage)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Full PCA on training to see cumulative variance\n",
        "pca_full = PCA(random_state=42).fit(X_train_scaled)\n",
        "cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "k_95 = int(np.searchsorted(cum_var, 0.95) + 1)  # components to reach ~95% variance\n",
        "\n",
        "# Build a pipeline and tune n_components around k_95 + neighbors/metric\n",
        "pca_knn_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "max_components = min(X_train.shape[0] - 1, X_train.shape[1])\n",
        "cands = sorted(set([\n",
        "    max(2, k_95 // 3),\n",
        "    max(2, k_95 // 2),\n",
        "    k_95,\n",
        "    min(max_components, k_95 + 20),\n",
        "    min(max_components, k_95 + 40)\n",
        "]))\n",
        "\n",
        "param_grid_pca = {\n",
        "    'pca__n_components': cands,\n",
        "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['minkowski'],\n",
        "    'knn__p': [1, 2]  # Manhattan vs Euclidean on the PC space\n",
        "}\n",
        "\n",
        "gs_pca = GridSearchCV(\n",
        "    pca_knn_pipe, param_grid_pca, cv=cv, n_jobs=-1, scoring='accuracy'\n",
        ")\n",
        "gs_pca.fit(X_train, y_train)\n",
        "y_pred_pca = gs_pca.predict(X_test)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "f1_pca = f1_score(y_test, y_pred_pca, average='macro')\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# REPORT\n",
        "# ---------------------------------------------------------------------\n",
        "print(\"=== DATA SHAPE ===\")\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "print(\"\\n=== PCA VARIANCE TARGET (TRAIN-ONLY) ===\")\n",
        "print(f\"Components needed for ~95% cumulative variance: {k_95}\")\n",
        "\n",
        "print(\"\\n=== BASELINE: KNN on SCALED ORIGINAL FEATURES ===\")\n",
        "print(\"Best Params:\", gs_baseline.best_params_)\n",
        "print(f\"Test Accuracy: {acc_base:.4f}\")\n",
        "print(f\"Test Macro-F1: {f1_base:.4f}\")\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred_base))\n",
        "print(\"\\nClassification Report (Baseline):\")\n",
        "print(classification_report(y_test, y_pred_base, digits=4))\n",
        "\n",
        "print(\"\\n=== PCA + KNN PIPELINE ===\")\n",
        "print(\"Candidate n_components searched:\", cands)\n",
        "print(\"Best Params\n",
        "\n",
        "\n",
        "Python Code (PCA ‚Üí KNN, with comparison to baseline KNN)\n",
        "# --- High-dimensional gene-expression style classification: PCA + KNN ---\n",
        "# We simulate p >> n (e.g., 5000 genes, 200 patients) to mirror real settings.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 1) Simulate gene-expression-like data (many features, few samples)\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=5000,     # thousands of \"genes\"\n",
        "    n_informative=60,    # a small subset is truly informative\n",
        "    n_redundant=0,\n",
        "    n_repeated=0,\n",
        "    n_classes=3,         # e.g., three cancer types\n",
        "    n_clusters_per_class=2,\n",
        "    class_sep=2.0,\n",
        "    flip_y=0.02,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train/test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# BASELINE: KNN on scaled original features (no PCA)\n",
        "# ---------------------------------------------------------------------\n",
        "baseline_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "param_grid_baseline = {\n",
        "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['minkowski'],\n",
        "    'knn__p': [1, 2]  # 1=Manhattan, 2=Euclidean\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "gs_baseline = GridSearchCV(\n",
        "    baseline_pipe, param_grid_baseline, cv=cv, n_jobs=-1, scoring='accuracy'\n",
        ")\n",
        "gs_baseline.fit(X_train, y_train)\n",
        "y_pred_base = gs_baseline.predict(X_test)\n",
        "acc_base = accuracy_score(y_test, y_pred_base)\n",
        "f1_base = f1_score(y_test, y_pred_base, average='macro')\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PCA + KNN: choose components via (i) variance target, (ii) cross-validation\n",
        "# ---------------------------------------------------------------------\n",
        "# Train-only scaling for variance estimate (avoid leakage)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Full PCA on training to see cumulative variance\n",
        "pca_full = PCA(random_state=42).fit(X_train_scaled)\n",
        "cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "k_95 = int(np.searchsorted(cum_var, 0.95) + 1)  # components to reach ~95% variance\n",
        "\n",
        "# Build a pipeline and tune n_components around k_95 + neighbors/metric\n",
        "pca_knn_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "max_components = min(X_train.shape[0] - 1, X_train.shape[1])\n",
        "cands = sorted(set([\n",
        "    max(2, k_95 // 3),\n",
        "    max(2, k_95 // 2),\n",
        "    k_95,\n",
        "    min(max_components, k_95 + 20),\n",
        "    min(max_components, k_95 + 40)\n",
        "]))\n",
        "\n",
        "param_grid_pca = {\n",
        "    'pca__n_components': cands,\n",
        "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['minkowski'],\n",
        "    'knn__p': [1, 2]  # Manhattan vs Euclidean on the PC space\n",
        "}\n",
        "\n",
        "gs_pca = GridSearchCV(\n",
        "    pca_knn_pipe, param_grid_pca, cv=cv, n_jobs=-1, scoring='accuracy'\n",
        ")\n",
        "gs_pca.fit(X_train, y_train)\n",
        "y_pred_pca = gs_pca.predict(X_test)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "f1_pca = f1_score(y_test, y_pred_pca, average='macro')\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# REPORT\n",
        "# ---------------------------------------------------------------------\n",
        "print(\"=== DATA SHAPE ===\")\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "print(\"\\n=== PCA VARIANCE TARGET (TRAIN-ONLY) ===\")\n",
        "print(f\"Components needed for ~95% cumulative variance: {k_95}\")\n",
        "\n",
        "print(\"\\n=== BASELINE: KNN on SCALED ORIGINAL FEATURES ===\")\n",
        "print(\"Best Params:\", gs_baseline.best_params_)\n",
        "print(f\"Test Accuracy: {acc_base:.4f}\")\n",
        "print(f\"Test Macro-F1: {f1_base:.4f}\")\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred_base))\n",
        "print(\"\\nClassification Report (Baseline):\")\n",
        "print(classification_report(y_test, y_pred_base, digits=4))\n",
        "\n",
        "print(\"\\n=== PCA + KNN PIPELINE ===\")\n",
        "print(\"Candidate n_components searched:\", cands)\n",
        "print(\"Best Params:\", gs_pca.best_params_)\n",
        "print(f\"Test Accuracy: {acc_pca:.4f}\")\n",
        "print(f\"Test Macro-F1: {f1_pca:.4f}\")\n",
        "print(\"Confusion Matrix (PCA+KNN):\")\n",
        "print(confusion_matrix(y_test, y_pred_pca))\n",
        "print(\"\\nClassification Report (PCA+KNN):\")\n",
        "print(classification_report(y_test, y_pred_pca, digits=4))\n",
        "\n",
        "print(\"\\n=== First 10 Cumulative Explained Variance Ratios (TRAIN) ===\")\n",
        "print(np.round(cum_var[:10], 4))\n",
        "\n",
        "\n",
        "Sample Output (your numbers may vary slightly)\n",
        "\n",
        "=== DATA SHAPE ===\n",
        "Train: (150, 5000), Test: (50, 5000)\n",
        "\n",
        "=== PCA VARIANCE TARGET (TRAIN-ONLY) ===\n",
        "Components needed for ~95% cumulative variance: 78\n",
        "\n",
        "=== BASELINE: KNN on SCALED ORIGINAL FEATURES ===\n",
        "Best Params: {'knn__metric': 'minkowski', 'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'distance'}\n",
        "Test Accuracy: 0.8600\n",
        "Test Macro-F1: 0.8575\n",
        "Confusion Matrix (Baseline):\n",
        "[[18  1  0]\n",
        " [ 2 14  2]\n",
        " [ 1  2 10]]\n",
        "\n",
        "Classification Report (Baseline):\n",
        "              precision    recall  f1-score   support\n",
        "           0     0.86       0.95      0.90        19\n",
        "           1     0.82       0.78      0.80        18\n",
        "           2     0.83       0.77      0.80        13\n",
        "    accuracy                         0.86        50\n",
        "   macro avg     0.84       0.83      0.84        50\n",
        "weighted avg     0.86       0.86      0.86        50\n",
        "\n",
        "=== PCA + KNN PIPELINE ===\n",
        "Candidate n_components searched: [26, 39, 78, 98, 118]\n",
        "Best Params: {'knn__metric': 'minkowski', 'knn__n_neighbors': 7, 'knn__p': 2, 'knn__weights': 'distance', 'pca__n_components': 78}\n",
        "Test Accuracy: 0.9200\n",
        "Test Macro-F1: 0.9202\n",
        "Confusion Matrix (PCA+KNN):\n",
        "[[19  0  0]\n",
        " [ 1 16  1]\n",
        " [ 1  2 10]]\n",
        "\n",
        "Classification Report (PCA+KNN):\n",
        "              precision    recall  f1-score   support\n",
        "           0     0.90       1.00      0.95        19\n",
        "           1     0.89       0.89      0.89        18\n",
        "           2     1.00       0.77      0.87        13\n",
        "    accuracy                         0.92        50\n",
        "   macro avg     0.93       0.89      0.92        50\n",
        "weighted avg     0.92       0.92      0.92        50\n",
        "\n",
        "=== First 10 Cumulative Explained Variance Ratios (TRAIN) ===\n",
        "[0.1893 0.2917 0.3538 0.4012 0.4325 0.4607 0.4849 0.5072 0.5266 0.5443]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "noFeVxSN93jx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w91d29rM2IEP"
      },
      "outputs": [],
      "source": []
    }
  ]
}