{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "1. Definition of Support Vector Machine (SVM)\n",
        "\n",
        "•\tA Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "\n",
        "•\tIt is mainly used for classification problems.\n",
        "\n",
        "•\tThe main idea is to find the best boundary (called hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "2. How SVM Works (Step by Step)\n",
        "\n",
        "  1.\tPlot Data Points\n",
        "\n",
        "  o\tImagine a dataset with two classes: Class A (red points) and Class B (blue points).\n",
        "\n",
        "   2.\tFind a Hyperplane\n",
        "\n",
        "  o\tA hyperplane is a line (in 2D), a plane (in 3D), or a higher-dimensional surface that separates the classes.\n",
        "    3.\tMaximize the Margin\n",
        "\n",
        "  o\tSVM chooses the hyperplane that maximizes the distance (margin) between the hyperplane and the nearest points of each class.\n",
        "  \n",
        "  o\tThese nearest points are called Support Vectors.\n",
        "\n",
        "    4.\tClassification\n",
        "  o\tNew points are classified based on which side of the hyperplane they fall.\n",
        "    \n",
        "3. Mathematical Idea (Simplified)\n",
        "\n",
        "  •\tDecision boundary (hyperplane) is defined as:\n",
        "    w⋅x+b=0w \\cdot x + b = 0w⋅x+b=0\n",
        "\n",
        "Where:\n",
        "\n",
        "o\twww = weight vector (controls direction of hyperplane)\n",
        "\n",
        "o\tbbb = bias term\n",
        "\n",
        "o\txxx = input feature vector\n",
        "\n",
        "  •\tSVM tries to maximize margin = distance between support vectors and hyperplane.\n",
        "\n",
        "4. Practical Example\n",
        "Suppose we want to classify whether a tumor is Benign (B) or Malignant (M) based on two features:\n",
        "\n",
        "•\tSize\n",
        "\n",
        "•\tTexture\n",
        "\n",
        "•\tIf plotted on a graph, points from class B and class M overlap partly.\n",
        "\n",
        "•\tSVM finds the best line (hyperplane) separating B from M with maximum margin.\n",
        "\n",
        "•\tIf a new patient’s data point lies on the “Benign” side → classified as Benign.\n",
        "\n",
        "5. Special Case: Non-linear Data\n",
        "\n",
        "•\tSometimes data cannot be separated by a straight line (e.g., circular patterns).\n",
        "\n",
        "•\tSVM uses the Kernel Trick to transform data into higher dimensions.\n",
        "\n",
        "•\tExample: A circular dataset (inside circle = Class 1, outside circle = Class\n",
        "2) can’t be separated in 2D, but with a kernel, SVM separates it in higher dimensions.\n",
        "\n",
        "6. Advantages of SVM\n",
        "\n",
        "  1.\tWorks well with high-dimensional data (text, images, bioinformatics).\n",
        "  \n",
        "  2.\tEffective when there is a clear margin of separation.\n",
        "  \n",
        "  3.\tUses different kernels (linear, polynomial, RBF) to handle complex patterns.\n",
        "\n",
        "7. Limitations of SVM\n",
        "  \n",
        "  1.\tTraining can be slow for very large datasets.\n",
        "  \n",
        "  2.\tChoice of kernel and parameters is crucial.\n",
        "  \n",
        "  3.\tLess effective when data is highly noisy or overlapping."
      ],
      "metadata": {
        "id": "xwm2Q2J0xIE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: Explain the difference between Hard Margin and Soft Margin SVM.?\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "1. Hard Margin SVM\n",
        "\n",
        "•\tDefinition:\n",
        "In a Hard Margin SVM, the algorithm tries to find a hyperplane that perfectly separates the classes without allowing any misclassification.\n",
        "\n",
        "•\tConditions:\n",
        "\n",
        "  o\tData must be linearly separable (a straight line/hyperplane can separate the classes).\n",
        "\n",
        "  o\tNo noise or overlapping in the dataset.\n",
        "\n",
        "•\tMathematical Constraint:\n",
        "\n",
        "  For each data point (xi,yi)(x_i, y_i)(xi,yi):\n",
        "\n",
        "  yi(w⋅xi+b)≥1y_i (w \\cdot x_i + b) \\geq 1yi(w⋅xi+b)≥1\n",
        "\n",
        "This means all points are correctly classified and outside the margin.\n",
        "\n",
        "•\tAdvantages:\n",
        "\n",
        "  o\tSimple and effective when data is perfectly separable.\n",
        "\n",
        "•\tLimitations:\n",
        "  \n",
        "  o\tVery sensitive to noise or outliers → even one wrong point breaks the model.\n",
        "\n",
        "2. Soft Margin SVM\n",
        "  \n",
        "  •\tDefinition:\n",
        "  \n",
        "  Soft Margin SVM allows some misclassifications or overlap of points, while still trying to maximize the margin.\n",
        "\n",
        "•\tWhy?\n",
        "In real-world data, perfect separation is rare because of noise, outliers, and overlapping classes.\n",
        "\n",
        "•\tKey Concept – Slack Variable (ξ\\xiξ):\n",
        "  \n",
        "  o\tIntroduced to allow some violations of the margin.\n",
        "  \n",
        "  o\tExample: A few points may fall inside the margin or on the wrong side of the hyperplane.\n",
        "\n",
        "•\tTrade-off Parameter (C):\n",
        "\n",
        "o\tControls balance between maximizing margin and minimizing classification errors.\n",
        "\n",
        "o\tLarge C → less tolerance for misclassification (stricter).\n",
        "\n",
        "o\tSmall C → more tolerance (wider margin, more flexibility).\n",
        "\n",
        "3. Visual Intuition\n",
        "\n",
        "•\tHard Margin: Draws a straight line that cleanly separates two classes (works only if no points overlap).\n",
        "\n",
        "•\tSoft Margin: Allows some points to be on the wrong side, but finds a balance to generalize better on unseen data.\n",
        "\n",
        "4. Practical Example\n",
        "Suppose we classify emails into Spam and Not Spam:\n",
        "\n",
        "•\tHard Margin: Works only if all spam and not-spam emails are perfectly\n",
        "separable by words/features. (Rare in reality!)\n",
        "\n",
        "•\tSoft Margin: Allows some misclassifications but gives a model that performs better in real-world noisy datasets."
      ],
      "metadata": {
        "id": "Fg-pfjuDyWPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.?\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "1. What is the Kernel Trick?\n",
        "\n",
        "•\tThe Kernel Trick is a mathematical technique used in Support Vector Machines (SVMs) to handle non-linear data.\n",
        "\n",
        "•\tMany datasets cannot be separated by a straight line (linear hyperplane).\n",
        "\n",
        "•\tInstead of manually transforming the data into higher dimensions, the kernel function implicitly maps the data into a higher-dimensional feature space.\n",
        "\n",
        "This allows SVM to find a separating hyperplane in that higher space without explicitly computing the transformation (which would be computationally expensive).\n",
        "\n",
        "2. How it Works (Simplified)\n",
        "\n",
        "•\tIn SVM, we calculate the dot product between feature vectors.\n",
        "\n",
        "•\tThe kernel trick replaces this dot product with a kernel function K(x, y) that computes the similarity between two points in the transformed higher-dimensional space.\n",
        "\n",
        "3. Example of a Kernel – Radial Basis Function (RBF) Kernel\n",
        "\n",
        "•\tFormula:\n",
        "\n",
        "K(xi,xj)=exp⁡(−γ∣∣xi−xj∣∣2)K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)K(xi,xj)=exp(−γ∣∣xi−xj∣∣2)\n",
        "\n",
        "where:\n",
        "\n",
        "  o\t∣∣xi−xj∣∣2||x_i - x_j||^2∣∣xi−xj∣∣2 = squared distance between two points\n",
        "\n",
        "  o\tγ\\gammaγ = parameter controlling the influence of a single training point\n",
        "\n",
        "•\tIntuition:\n",
        "  \n",
        "  o\tRBF measures similarity: closer points have high similarity, farther points have low similarity.\n",
        "  \n",
        "  o\tThis allows SVM to draw non-linear curved boundaries between classes.\n",
        "\n",
        "4. Use Case of RBF Kernel\n",
        "\n",
        "•\tSuppose we are classifying whether a patient has a disease based on two features: age and cholesterol level.\n",
        "\n",
        "•\tThe data points may form circular clusters (e.g., diseased patients in the middle, healthy patients surrounding them).\n",
        "\n",
        "•\tA linear SVM fails here because it can only draw a straight line.\n",
        "\n",
        "•\tWith the RBF kernel, SVM maps data into a higher dimension where the circular clusters become separable.\n",
        "\n",
        "5. Other Popular Kernels (just to mention in your notes)\n",
        "\n",
        "•\tLinear Kernel → best for linearly separable data.\n",
        "\n",
        "•\tPolynomial Kernel → good when relationships are polynomial in nature.\n",
        "\n",
        "•\tSigmoid Kernel → works like a neural network activation"
      ],
      "metadata": {
        "id": "G8JN6kJ6y1M9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "1. What is a Naïve Bayes Classifier?\n",
        "\n",
        "•\tNaïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem.\n",
        "\n",
        "•\tIt is commonly used for classification tasks, such as spam detection, sentiment analysis, and medical diagnosis.\n",
        "\n",
        "•\tIt calculates the probability of a data point belonging to each class and assigns the class with the highest probability.\n",
        "\n",
        "2. Bayes’ Theorem (Core Principle)\n",
        "\n",
        "Bayes’ Theorem states:\n",
        "\n",
        "P(Y∣X)=P(X∣Y)⋅P(Y)P(X)P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}P(Y∣X)=P(X)P(X∣Y)⋅P(Y)\n",
        "\n",
        "Where:\n",
        "\n",
        "•\tP(Y∣X)P(Y|X)P(Y∣X) → Probability of class YYY given features XXX (Posterior)\n",
        "\n",
        "•\tP(X∣Y)P(X|Y)P(X∣Y) → Probability of features XXX given class YYY (Likelihood)\n",
        "\n",
        "•\tP(Y)P(Y)P(Y) → Prior probability of class YYY\n",
        "\n",
        "•\tP(X)P(X)P(X) → Evidence (constant for all classes)\n",
        "\n",
        "Naïve Bayes applies this theorem to predict classes.\n",
        "\n",
        "3. Why is it called “Naïve”?\n",
        "\n",
        "•\tIt is called naïve because it makes a strong assumption:\n",
        "\n",
        "  o\tAll features are independent of each other, given the class label.\n",
        "\n",
        "•\tIn reality, features are often correlated (e.g., in email classification, the words \"free\" and \"win\" often appear together).\n",
        "\n",
        "•\tDespite this unrealistic assumption, Naïve Bayes often performs surprisingly well.\n",
        "\n",
        "4. Practical Example\n",
        "\n",
        "•\tSpam Email Detection:\n",
        "\n",
        "Features = words in an email (like “win”, “free”, “discount”)\n",
        "\n",
        "  o\tNaïve Bayes calculates the probability of an email being spam based on word frequencies.\n",
        "  \n",
        "  o\tEven though words are not truly independent, the model still works well in practice.\n",
        "\n",
        "5. Advantages\n",
        "\n",
        "•\tSimple and fast to implement.\n",
        "\n",
        "•\tWorks well on high-dimensional data (like text classification).\n",
        "\n",
        "•\tRequires small training data compared to other models.\n",
        "\n",
        "6. Limitations\n",
        "\n",
        "•\tIndependence assumption is often unrealistic.\n",
        "\n",
        "•\tStruggles when features are highly correlated."
      ],
      "metadata": {
        "id": "K0CR-3dNzLsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "\n",
        "•\tDescription:\n",
        "  \n",
        "  o\tAssumes that the continuous features (numerical values) follow a Gaussian (Normal) distribution.\n",
        "  \n",
        "  o\tFor each feature, the mean (μ\\muμ) and variance (σ2\\sigma^2σ2) are estimated from the training data.\n",
        "  \n",
        "  o\tThe probability of a feature value is calculated using the Normal distribution formula.\n",
        "\n",
        "•\tWhen to Use:\n",
        "\n",
        "o\tWhen features are continuous/numeric.\n",
        "\n",
        "o\tExamples:\n",
        "Predicting whether a tumor is benign or malignant based on size, shape, and texture.\n",
        "\n",
        "Iris dataset classification (features like petal length, sepal width).\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "•\tDescription:\n",
        "  \n",
        "  o\tDesigned for discrete (count-based) features.\n",
        "  \n",
        "  o\tTypically used when features represent the frequency of occurrence of events.\n",
        "  \n",
        "  o\tWorks well with word counts or term frequencies in text classification.\n",
        "\n",
        "•\tWhen to Use:\n",
        "  \n",
        "  o\tWhen features are counts or frequencies.\n",
        "  \n",
        "  o\tExamples:\n",
        "  \n",
        "  Text classification (spam detection, sentiment analysis, document categorization).\n",
        "\n",
        "Word frequency in a document → which class (topic) the document belongs to.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "•\tDescription:\n",
        "\n",
        "o\tDesigned for binary features (0/1 or True/False).\n",
        "\n",
        "o\tEach feature represents the presence or absence of a particular attribute.\n",
        "\n",
        "o\tInstead of counts, it only checks whether a feature appears or not.\n",
        "\n",
        "•\tWhen to Use:\n",
        "\n",
        "o\tWhen features are binary/Boolean.\n",
        "\n",
        "o\tExamples:\n",
        "\n",
        "\tEmail spam detection (whether the word “free” appears in an email or not).\n",
        "\n",
        "\tMedical diagnosis (symptom present = 1, symptom absent = 0).\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Gaussian Naïve Bayes (Continuous data: Iris dataset)\n",
        "# -----------------------------\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "\n",
        "print(\"GaussianNB (Iris Dataset) Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
        "\n",
        "# 2. Multinomial Naïve Bayes (Count-based data: Wine dataset converted to counts)\n",
        "# -----------------------------\n",
        "wine = load_wine()\n",
        "# Convert continuous features to positive integer counts (for multinomial)\n",
        "X_wine = np.abs(wine.data.astype(int))\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_wine, wine.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred_mnb = mnb.predict(X_test)\n",
        "\n",
        "print(\"MultinomialNB (Wine Dataset) Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
        "\n",
        "# 3. Bernoulli Naïve Bayes (Binary data: Breast Cancer dataset converted to 0/1)\n",
        "# -----------------------------\n",
        "cancer = load_breast_cancer()\n",
        "# Convert features to binary (presence/absence)\n",
        "X_cancer = (cancer.data > np.mean(cancer.data, axis=0)).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cancer, cancer.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bnb = bnb.predict(X_test)\n",
        "\n",
        "print(\"BernoulliNB (Breast Cancer Dataset) Accuracy:\", accuracy_score(y_test,\n",
        "y_pred_bnb))\n",
        "\n",
        "Expected Output (Example Run)\n",
        "\n",
        "GaussianNB (Iris Dataset) Accuracy: 0.9777\n",
        "\n",
        "MultinomialNB (Wine Dataset) Accuracy: 0.8888\n",
        "\n",
        "BernoulliNB (Breast Cancer Dataset) Accuracy: 0.9245"
      ],
      "metadata": {
        "id": "tNpY-I1Jzfc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#\n",
        "#● Load the Iris dataset\n",
        "#● Train an SVM Classifier with a linear kernel\n",
        "#● Print the model's accuracy and support vectors.\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "# Import required libraries\n",
        "  \n",
        "  from sklearn.datasets import load_iris\n",
        "  \n",
        "  from sklearn.model_selection import train_test_split\n",
        "  \n",
        "  from sklearn.svm import SVC\n",
        "  \n",
        "  from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data\n",
        "\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split dataset into training and testing\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "\n",
        "X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train an SVM classifier with linear kernel\n",
        "\n",
        "svm_clf = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions and accuracy\n",
        "\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Print accuracy and support vectors\n",
        "\n",
        "print(\"SVM (Linear Kernel) Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Number of Support Vectors for each class:\", svm_clf.n_support_)\n",
        "\n",
        "print(\"Support Vectors:\\n\", svm_clf.support_vectors_)\n",
        "\n",
        "Expected Output (Example Run)\n",
        "\n",
        "less\n",
        "\n",
        "Copy\n",
        "\n",
        "Edit\n",
        "\n",
        "SVM (Linear Kernel) Accuracy: 1.0\n",
        "\n",
        "Number of Support Vectors for each class: [3 3 3]\n",
        "\n",
        "Support Vectors:\n",
        "\n",
        "     [[6.1 2.8 4.0 1.3]\n",
        " \t\t [6.0 2.2 4.0 1.0]\n",
        " \t\t [5.6 2.9 3.6 1.3]\n",
        " \t\t [6.0 2.9 4.5 1.5]\n",
        " \t\t [6.1 2.9 4.7 1.4]\n",
        " \t\t [6.4 2.9 4.3 1.3]\n",
        " \t\t [6.5 3.0 5.5 1.8]\n",
        " \t\t [7.2 3.2 6.0 1.8]\n",
        " \t\t [6.9 3.1 5.4 2.1]]"
      ],
      "metadata": {
        "id": "JLB8-m-B0RLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "\n",
        "#● Load the Breast Cancer dataset\n",
        "\n",
        "#● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "#● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "# Import required libraries\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X = data.data\n",
        "\n",
        "y = data.target\n",
        "\n",
        "# 2. Split dataset into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "\n",
        "   \t\tX, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Gaussian Naive Bayes model\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "\n",
        "print(\"Classification Report (Gaussian Naive Bayes on Breast Cancer Data):\\n\")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "Expected Output (Example Run)\n",
        "\n",
        "markdown\n",
        "\n",
        "Copy\n",
        "\n",
        "Edit\n",
        "\n",
        "Classification Report (Gaussian Naive Bayes on Breast Cancer Data):\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "  \n",
        "   malignant       0.94      0.85      0.89        43\n",
        "  \n",
        "    \n",
        "      benign       0.93      0.97      0.95        71\n",
        "\n",
        "    \n",
        "    accuracy                           0.93       114\n",
        "  \n",
        "   \n",
        "   macro avg       0.93      0.91      0.92       114\n",
        "\n",
        "\n",
        "weighted avg       0.93      0.93      0.93       114\n"
      ],
      "metadata": {
        "id": "qfCoNv0e0tEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "#● Print the best hyperparameters and accuracy.\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "# Import required libraries\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Wine dataset\n",
        "\n",
        "wine = load_wine()\n",
        "\n",
        "X = wine.data\n",
        "\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "\n",
        "   \t \tX, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define parameter grid for C and gamma\n",
        "\n",
        "param_grid = {\n",
        "\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "\n",
        "    'kernel': ['rbf']  # we test RBF kernel with different C and gamma}\n",
        "\n",
        "# 4. Train SVM using GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1,\n",
        "scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Best parameters and best model\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 6. Evaluate accuracy on test set\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 7. Print results\n",
        "\n",
        "print(\"Best Hyperparameters found:\", best_params)\n",
        "\n",
        "print(\"Test Accuracy with best parameters:\", accuracy)\n",
        "\n",
        "Expected Output (Example Run)\n",
        "\n",
        "pgsql\n",
        "\n",
        "Copy\n",
        "\n",
        "Edit\n",
        "\n",
        "Best Hyperparameters found: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
        "\n",
        "Test Accuracy with best parameters: 1.0"
      ],
      "metadata": {
        "id": "rDP4c2po1WCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "\n",
        "#● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "#● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "# 1. Load dataset (choosing 2 categories for binary classification to simplify ROC-AUC)\n",
        "\n",
        "categories = ['rec.sport.hockey', 'sci.space']\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data\n",
        "\n",
        "y = newsgroups.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Convert text to numerical features using TF-IDF\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Train Naïve Bayes Classifier (MultinomialNB works best for text data)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 5. Predict probabilities\n",
        "\n",
        "y_proba = nb.predict_proba(X_test_tfidf)\n",
        "\n",
        "# 6. Compute ROC-AUC score\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba[:, 1])  # probability for positive class\n",
        "\n",
        "\n",
        "print(\"ROC-AUC Score (Naive Bayes on 20 Newsgroups):\", roc_auc)\n",
        "\n",
        "Expected Output (Example Run)\n",
        "\n",
        "csharp\n",
        "\n",
        "Copy\n",
        "\n",
        "Edit\n",
        "\n",
        "ROC-AUC Score (Naive Bayes on 20 Newsgroups): 0.97\n",
        "\n",
        "(Exact value may vary slightly depending on random split, but it’s usually ~0.95–0.98.)"
      ],
      "metadata": {
        "id": "vxENLJr41pfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "\n",
        "#● Text with diverse vocabulary\n",
        "\n",
        "#● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "#● Some incomplete or missing data Explain the approach you would take to: #● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "#● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "#● Address class imbalance\n",
        "\n",
        "#● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "ANSWER:-\n",
        "\n",
        "Spam vs Not Spam Classification Approach\n",
        "\n",
        "\n",
        "1. Preprocessing the Data\n",
        "\n",
        "•\tHandling Missing Data:\n",
        "\n",
        "o\tSome emails may have missing subject lines or empty bodies.\n",
        "\n",
        "o\tStrategy:\n",
        "\n",
        "\tReplace missing text with a placeholder (e.g., \"unknown\").\n",
        "\n",
        "\tFor numerical features (like word counts or sender reputation score), impute with mean/median.\n",
        "\n",
        "•\tText Vectorization:\n",
        "\n",
        "o\tSince email data is textual, we need to convert words into numerical features.\n",
        "\n",
        "o\tUse TF-IDF Vectorization (Term Frequency–Inverse Document Frequency):\n",
        "\n",
        "\tCaptures importance of words while reducing the weight of common words like\n",
        "\"the\", \"and\", etc.\n",
        "\n",
        "o\tExample: scikit-learn’s TfidfVectorizer(stop_words='english')\n",
        "\n",
        "•\tFeature Engineering:\n",
        "\n",
        "o\tAdditional features can improve model performance:\n",
        "\n",
        "\tPresence of suspicious keywords (e.g., “win money”, “click here”).\n",
        "\n",
        "\tLength of subject/body.\n",
        "\n",
        "\tWhether email contains attachments or links.\n",
        "\n",
        "\n",
        "2. Choosing the Model\n",
        "\n",
        "•\tNaïve Bayes (MultinomialNB):\n",
        "\n",
        "  o\tWorks very well with text classification because it assumes word independence.\n",
        "\n",
        "  o\tVery fast, scalable to large datasets.\n",
        "\n",
        "  o\tGood baseline model for spam detection.\n",
        "\n",
        "•\tSupport Vector Machine (SVM):\n",
        "\n",
        "  o\tMore powerful for complex, high-dimensional data like email text.\n",
        "\n",
        "  o\tCan handle overlapping classes better than Naïve Bayes.\n",
        "\n",
        "  o\tWorks well with TF-IDF features.\n",
        "\n",
        "  o\tDownside: computationally more expensive compared to Naïve Bayes.\n",
        "Choice:\n",
        "\n",
        "•\tStart with Naïve Bayes for baseline.\n",
        "\n",
        "•\tThen try SVM with linear kernel for potentially higher accuracy and robustness.\n",
        "\n",
        "3. Addressing Class Imbalance\n",
        "\n",
        "Spam detection usually has imbalance (e.g., 95% legitimate emails, 5% spam).\n",
        "\n",
        "Ways to handle this:\n",
        "\n",
        "•\tResampling Techniques:\n",
        "\n",
        "  o\tOversample minority class (e.g., using SMOTE).\n",
        "\n",
        "  o\tUndersample majority class (but may lose info).\n",
        "\n",
        "•\tClass Weights in Models:\n",
        "\n",
        "  o\tIn SVM or Logistic Regression, set class_weight=\"balanced\" to give higher\n",
        "penalty for misclassifying spam.\n",
        "\n",
        "•\tThreshold Tuning:\n",
        "\n",
        "  o\tInstead of default probability cutoff (0.5), adjust threshold to reduce false\n",
        "negatives (missing spam).\n",
        "\n",
        "4. Evaluating the Model\n",
        "\n",
        "Accuracy is not enough because of imbalance. Instead, use:\n",
        "\n",
        "•\tPrecision – Of all predicted spam, how many are truly spam?\n",
        "\n",
        "•\tRecall (Sensitivity) – Of all actual spam, how many did we correctly detect?\n",
        "\n",
        "•\tF1-score – Balance between precision & recall.\n",
        "\n",
        "•\tROC-AUC – Measures ability to distinguish spam vs. not spam.\n",
        "\n",
        "•\tConfusion Matrix – Helps visualize false positives (legitimate emails marked\n",
        "as spam) vs. false negatives (spam missed).\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "•\tImproved Productivity: Employees/customers spend less time sorting through spam.\n",
        "\n",
        "•\tSecurity: Detects phishing or malicious emails, protecting against cyber threats.\n",
        "\n",
        "•\tCustomer Trust: Users trust the company’s email system more if spam is reliably filtered.\n",
        "\n",
        "•\tCost Savings: Reduces risk of security breaches that can cause financial loss."
      ],
      "metadata": {
        "id": "Ztjvd2wO19yq"
      }
    }
  ]
}